{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://thevalley.es/wp-content/uploads/2016/11/thevalley-logo-negro.png\" width=\"400\"></img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "El algoritmo de los K-Nearest-Neighbors o *K-Vecinos* es un algoritmo basado en instancia (o vago) de tipo supervisado de Machine Learning. Puede usarse para clasificar elemementos (valores discretos) o para predecir valores continuos(regresión, valores continuos). \n",
    "Es un algoritmo simple, fácil de entender, versátil y uno de los mejores de aprendizaje automático. KNN se utiliza en una amplia gama de aplicaciones como finanzas, salud, ciencias políticas, detección de escritura, reconocimiento de imágenes y video. \n",
    "\n",
    "El algoritmo KNN basado se basa en predecir un valor o una clase en base a las K instancias más parecidas.\n",
    "\n",
    "## Algoritmos vagos versus voraces\n",
    "\n",
    "Los algoritmos voraces (o ávidos de datos) son capaces de contruir un modelo  antes de realizar predicciones sobre nuevos puntos dados para clasificar. Puede pensar en estos algoritmos como si estuvieran preparados, activos y deseosos de clasificar los puntos de datos no observados.\n",
    "\n",
    "Los algoritmos vagos (o perezosos) significa que no hay necesidad de aprender o entrenar el modelo y todos los puntos de datos usados en el momento de la predicción. Los estudiantes perezosos esperan hasta el último minuto antes de clasificar cualquier punto de datos. El alumno perezoso almacena simplemente el conjunto de datos de entrenamiento y espera hasta que se deba realizar la clasificación. Solo cuando ve la tupla de prueba, realiza una generalización para clasificar la tupla en función de su similitud con las tuplas de entrenamiento almacenadas. A diferencia de los métodos de aprendizaje ávidos, los estudiantes perezosos hacen menos trabajo en la fase de formación y más trabajo en la fase de prueba para hacer una clasificación. Los estudiantes perezosos también se conocen como estudiantes basados ​​en instancias porque los estudiantes perezosos almacenan los puntos o instancias de capacitación, y todo el aprendizaje se basa en instancias.\n",
    "\n",
    "## El algoritmo KNN\n",
    "KNN es un algoritmo de aprendizaje vago y no paramétrico. No paramétrico significa que no hay suposiciones para la distribución de datos subyacente. En otras palabras, la estructura del modelo determinada a partir del conjunto de datos. Esto será muy útil en la práctica donde la mayoría de los conjuntos de datos del mundo real no siguen supuestos teóricos matemáticos. El algoritmo perezoso significa que no necesita ningún punto de datos de entrenamiento para la generación del modelo. Todos los datos de entrenamiento utilizados en la fase de prueba. Esto hace que el entrenamiento sea más rápido y la fase de prueba más lenta y costosa. La costosa fase de prueba significa tiempo y memoria. En el peor de los casos, KNN necesita más tiempo para escanear todos los puntos de datos y escanear todos los puntos de datos requerirá más memoria para almacenar los datos de entrenamiento.\n",
    "\n",
    "![Imagen](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/Knn_k1_z96jba.png)\n",
    "\n",
    "Suponga que P1 es el punto, para el cual la etiqueta necesita predecir. Primero, encuentra el k punto más cercano a P1 y luego clasifica los puntos por voto mayoritario de sus k vecinos. Cada objeto vota por su clase y la clase con más votos se toma como predicción. Para encontrar puntos similares más cercanos, encuentre la distancia entre puntos utilizando medidas de distancia como la distancia euclidiana, la distancia de Hamming, la distancia de Manhattan y la distancia de Minkowski. \n",
    "\n",
    "Nuestro algoritmo sigue estos tres pasos:\n",
    "\n",
    "* Calcular la distancia\n",
    "* Encuentra los K vecinos más cercanos \n",
    "* Votación para la selección de la clase \n",
    "\n",
    "![Imagen](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final1_ibdm8a.png)\n",
    "\n",
    "\n",
    "### ¿Cómo funciona el algoritmo KNN?\n",
    "\n",
    "En KNN, K es el número de vecinos más cercanos. El número de vecinos es el factor decisivo fundamental. K es generalmente un número impar si el número de clases es 2. Cuando K = 1, entonces el algoritmo se conoce como el algoritmo de vecino más cercano. Este es el caso más simple. Suponga que P1 es el punto, para el cual la etiqueta necesita predecir. Primero, busque el punto más cercano a P1 y luego la etiqueta del punto más cercano asignado a P1.\n",
    "\n",
    "### La maldición de la dimensionalidad\n",
    "\n",
    "KNN funciona mejor con una menor cantidad de funciones que una gran cantidad de funciones. Puede decir que cuando aumenta el número de funciones, se requieren más datos. El aumento de la dimensión también conduce al problema del sobreajuste. Para evitar el sobreajuste, los datos necesarios deberán crecer exponencialmente a medida que aumente el número de dimensiones. Este problema de dimensión superior se conoce como la maldición de la dimensionalidad.\n",
    "\n",
    "Para lidiar con el problema de la maldición de la dimensionalidad, debe realizar un análisis de componentes principales antes de aplicar cualquier algoritmo de aprendizaje automático, o también puede usar el enfoque de selección de características. La investigación ha demostrado que en grandes dimensiones la distancia euclidiana ya no es útil. Por lo tanto, puede preferir otras medidas, como la similitud del coseno, que se ve claramente menos afectada por la dimensión alta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjuntos de datos\n",
    "(https://www.openml.org/s/76/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de librerías\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Una primera aproximación al algoritmo de k-vecinos\n",
    "\n",
    "Vamos a hacer una primera prueba del algoritmo de k-vecinos con un primer clasificador de animales en función del peso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gato']\n",
      "['tigre']\n",
      "[[1. 0.]]\n",
      "[[0.33333333 0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "Peso = [[2.5], [1.3], [0.8], [3.3],[120],[150],[210]]\n",
    "# 1 -> Gato\n",
    "# 2 -> Tigre\n",
    "clase = ['gato', 'gato', 'gato', 'gato', 'tigre', 'tigre','tigre']\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(Peso, clase)\n",
    "print(neigh.predict([[2.3]]))\n",
    "print(neigh.predict([[199]]))\n",
    "# Estimamos la probabilidad par aun animal de 2.3 kg\n",
    "print(neigh.predict_proba([[2.3]]))\n",
    "# Y si nos encontramos un animal del 80kg\n",
    "print(neigh.predict_proba([[80]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jugando con las medidas de distancia\n",
    "\n",
    "Vamos a poner a prueba nuestros conocimientos uniform le da la misma importancia a todas las instancias *(weights=uniform)*, que es el valor por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gato']\n",
      "['gato']\n"
     ]
    }
   ],
   "source": [
    "Peso = [[2.5], [1.3], [0.8], [3.3],[190]]\n",
    "clase = ['gato', 'gato', 'gato', 'gato', 'tigre']\n",
    "neigh = KNeighborsClassifier(n_neighbors=3, weights='uniform')\n",
    "neigh.fit(Peso, clase)\n",
    "print(neigh.predict([[2.3]]))\n",
    "print(neigh.predict([[199]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pregunta)* ¿Qué está pasando ....?\n",
    "\n",
    "Ahora vamos a probar con una versión del método que asigna un peso mayor a la instancia cuanto más cerca este de nuestro dato de entrada *(weights=distance)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gato']\n",
      "['tigre']\n"
     ]
    }
   ],
   "source": [
    "Peso = [[2.5], [1.3], [0.8], [3.3],[190]]\n",
    "clase = ['gato', 'gato', 'gato', 'gato', 'tigre']\n",
    "neigh = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
    "neigh.fit(Peso, clase)\n",
    "print(neigh.predict([[2.3]]))\n",
    "print(neigh.predict([[199]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pregunta)* ¿Y por qué ahora funciona *bien*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de clasificación multiclase\n",
    "\n",
    "Hasta ahora, ha aprendido a crear un clasificador KNN para dos en python usando scikit-learn. Ahora aprenderá sobre KNN con múltiples clases.\n",
    "\n",
    "En el modelo de la parte de construcción, puede utilizar el conjunto de datos de vino, que es un problema de clasificación de clases múltiples muy famoso. Estos datos son el resultado de un análisis químico de vinos cultivados en la misma región en Italia utilizando tres cultivares diferentes. El análisis determinó las cantidades de 13 componentes que se encuentran en cada uno de los tres tipos de vinos.\n",
    "\n",
    "El conjunto de datos comprende 13 características ('alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline')  y un objetivo (tipo de cultivares).\n",
    "\n",
    "Estos datos tienen tres tipos de clases de cultivares: 'class_0', 'class_1' y 'class_2'. Aquí, puede construir un modelo para clasificar el tipo de cultivo. El conjunto de datos está disponible en la biblioteca scikit-learn, o también puede descargarlo de la biblioteca de aprendizaje automático de UCI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos\n",
    "\n",
    "Cargamos los datos del paquete sckitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración básica\n",
    "\n",
    "Una vez que haya cargado el conjunto de datos, es posible que desee saber un poco más sobre él. Puede comprobar los nombres de las funciones y los objetivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n"
     ]
    }
   ],
   "source": [
    "# Mostrar características\n",
    "print(wine.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class_0' 'class_1' 'class_2']\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las clases objetivo\n",
    "print(wine.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos las 3 primeras instancias del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      "  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n",
      "  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n",
      "  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]]\n"
     ]
    }
   ],
   "source": [
    "print(wine.data[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check records of the target set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore it for a bit more. You can also check the shape of the dataset using shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de conjuntos de entrenamiento y validación\n",
    "\n",
    "Para comprender el rendimiento del modelo, dividir el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba es una buena estrategia.\n",
    "\n",
    "Dividamos el conjunto de datos usando la función train_test_split (). Debe pasar 3 parámetros, características, destino y tamaño del conjunto de pruebas. Además, puede usar random_state para seleccionar registros al azar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set 70% train/ 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación del modelo para K=5\n",
    "Elegimos una K que se nos ocurra, por ejemplo K=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create KNN Classifier\n",
    "K=5\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "#Train the model using the training sets\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del modelo para k = 5\n",
    "Calculemos con qué precisión el clasificador o modelo puede predecir el tipo de cultivares.\n",
    "\n",
    "La precisión se puede calcular comparando los valores reales del conjunto de prueba y los valores predichos.\n",
    "\n",
    "La exactitud es una métrica para evaluar modelos de clasificación. Informalmente, la exactitud es la fracción de predicciones que el modelo realizó correctamente. Formalmente, la exactitud tiene la siguiente definición:\n",
    "\n",
    "En la clasificación binaria, la exactitud (accuracy) también se puede calcular en términos de positivos y negativos de la siguiente manera:\n",
    "$$\n",
    "    Accuracy = \\frac{Verdaderos Positivos + Verdaderos Negativos}{Total\\ casos}\n",
    "$$\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/classification/accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6296296296296297\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo se decide el número de vecinos en KNN?\n",
    "\n",
    "Ahora que ya entendemos el funcionamiento del algoritmo, surge la pregunta ¿Cómo elegir el número óptimo de vecinos? ¿Y cuáles son sus efectos sobre el clasificador? El número de vecinos (K) en KNN es un hiperparámetro que debe elegir en el momento de la construcción del modelo. Puede pensar en K como una variable de control para el modelo de predicción.\n",
    "\n",
    "La investigación ha demostrado que no existe un K óptimo de vecinos que se ajuste a todo tipo de conjuntos de datos. Cada conjunto de datos tiene sus propios requisitos. En el caso de un pequeño número de vecinos, el ruido tendrá una mayor influencia en el resultado y un gran número de vecinos lo hace computacionalmente costoso. La investigación también ha demostrado que una pequeña cantidad de vecinos tienen un ajuste más flexible, lo que tendrá un sesgo bajo pero una varianza alta, y un gran número de vecinos tendrá un límite de decisión más suave, lo que significa una varianza menor pero un sesgo más alto.\n",
    "\n",
    "Generalmente, los científicos de datos eligen como un número impar si el número de clases es par. También puede verificar generando el modelo en diferentes valores de ky verificar su desempeño. También puedes probar el método del codo aquí.\n",
    "\n",
    "![Imagen](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final_a1mrv9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud (accuracy) K=3:  0.7407407407407407\n",
      "Exactitud (accuracy) K=4:  0.7222222222222222\n",
      "Exactitud (accuracy) K=5:  0.7037037037037037\n",
      "Exactitud (accuracy) K=6:  0.7037037037037037\n",
      "Exactitud (accuracy) K=7:  0.7222222222222222\n",
      "Exactitud (accuracy) K=8:  0.7222222222222222\n",
      "Exactitud (accuracy) K=9:  0.6851851851851852\n",
      "Mejor según accuracy: K=3\n"
     ]
    }
   ],
   "source": [
    "#Create KNN Classifier\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "for K in range(3,10):\n",
    "    knn = KNeighborsClassifier(n_neighbors=K)\n",
    "\n",
    "    #Train the model using the training sets\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = knn.predict(X_test)\n",
    "    #Import scikit-learn metrics module for accuracy calculation\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"Exactitud (accuracy) K=\"+str(K)+\": \",accuracy)\n",
    "    if best_accuracy < accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_k = K\n",
    "print('Mejor según accuracy: K=' + str(best_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo de sensibles somos a la falta de información?\n",
    "Estos algoritmos son muy sensibles a la falta de información, ¿estamos seguros de que estos resultados son correctos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor según accuracy: K=6 en iteración:9\n"
     ]
    }
   ],
   "source": [
    "best_iteration = 0\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "for iteration in range(1,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3) \n",
    "    for K in range(3,10):\n",
    "        knn = KNeighborsClassifier(n_neighbors=K)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "        #print(\"Exactitud [\"+str(iteration)+\"] (accuracy) K=\"+str(K)+\": \",accuracy)\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_k = K\n",
    "            best_iteration = iteration\n",
    "            \n",
    "print('Mejor según accuracy: K=' + str(best_k)+ ' en iteración:' + str(iteration))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas**: \n",
    "* *¿Qué puede estar pasando?*\n",
    "* *¿que pasa si cambiamos el test_size a un 10% o a un 50%?, qué pasa en cada caso*\n",
    "* *¿Cómo podríamos tener un mecanismo más robusto para la selección de la K?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efecto de la optimización en la búsqueda (ball tree, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exploring Data\n",
    "After you have loaded the dataset, you might want to know a little bit more about it. You can check feature and target names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar brute force versus kdtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "[K-Neighbors Scikitlearn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios propuestos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador de zonas gemelas\n",
    "Tenemos un listado de zonas en el municipio de Madrid, cada zona tiene una serie de características numéricas, eliminamos del conjunto de entrada los barrios Sol, ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediciendo la felicidad\n",
    "\n",
    "Podemos predecir la felicidad de un país en base a sus características, el ejercicio consiste en eliminar Japón, España, Alemania y Argelia del conjunto de entrenamiento y crear un modelo de k-vecinos de regresión con el resto de instancias. Una una vez lo tenemos tenemos que calcular la fiabilidad  de nuestro modelo con dos tres métricas:\n",
    "\n",
    "* Error medio\n",
    "* Error mediano\n",
    "* Error cuadrático medio\n",
    "\n",
    "El error lo calculamos como *nivel de felicidad predicho - nivel de felicidad real*\n",
    "\n",
    "*Pregunta 1)* ¿Con que número k funciona mejor el algoritmo?\n",
    "\n",
    "*Pregunta 2)*¿Cual es el efecto de usar pesos inversamente proporcionales a la distancia a no usarlos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
